---
layout: post
title: 4 Ways AI can lead to the end of Humankind
date: 2014-02-14 14:42:50.000000000 -05:00
type: post
published: true
status: publish
categories:
- Society
category: Society
tags:
- AI
- AI-pocalypse
- Apocalypse
- Artificial Intelligence
- End of the World
- Technology
- Technology Take-Over
author: insidious
---
<blockquote><p> The AI does not hate you, nor does  it love you, but you are made out of atoms which it can use for something else </p></blockquote> -Nick Bostrom


<p>Obvious advances in technology (just think of Boston Dynamics, Google Glass, or NEST) happen everyday, and with them questions of privacy, which seem to be a large focus of news outlet's these days. With the discovery of the NSA spying not only on it's citizens but virtually anyone with a smartphone, it may seem that we already live in an Orwellian society. AI, however, is a completely different yet ultimately more dangerous concept, and one which can get ugly -- fast. Like, Matrix ugly.</p>
<p>Think of the most powerful supercomputer you can; now multiply the power of that supercomputer by infinity and you have your Artificial Super Intelligence, or ASI for short. The supercomputer shares no emotion with humans, because it can't. How can you expect a computer to comprehend love, compassion, sympathy, sadness or happiness? Yet this computer has more intelligence then one could ever dream of, no less comprehend. Programmers thought they were smart by "programming" it with "friendliness"; but the computer cannot feel, it can only have boundaries, and what's stopping it from getting around those boundaries? not much. So I made a list about 4 ways which supercomputers might just wipe out all of humanity.</p>
<h3><strong>1. AI will Always Find a Way Around</strong></h3>
<a class="lightbox" href="{{site.url}}uploads/2014/02/matrix.gif"><img class="alignleft size-medium wp-image-54" src="{{site.url}}uploads/oldImg/matrix-300x240.gif" alt="matrix" width="300" height="240" /></a><p>So, you may be thinking at this point: "but why can't we just develop AI in a closed environment, that way, if something gets out of control we can just destroy it before it destroy's us". Sure - dealing with things in a contained environment is a great way to deal with pretty much anything potentially harmful, or in this case, humankind-extinction harmful. One must understand, however, that AI isn't just some harmful radiation - it's smart on a <em>human </em>level, super AI would be smart on a level beyond that which no human on earth can <strong>imagine</strong>. Which means we would not be dealing with vulnerabilities in a 100-foot thick steel vault, but rather vulnerabilities with humans themselves. Just take a look at the <a href="{{site.url}}uploads/2014/02/aibox">AI Box Experiment</a>. In short, there was a series of experiments, created by Eliezer Yudkowsky. In the experiments, Yudkowsky plays the AI, while another person plays the "gatekeeper". These two people conversed over a IRC chat. The maximum amount of time this game went on was 2 hours. If the AI got out, the AI wins. Between 2002 and 2005, Yudkowsky played this game 5 times, with 5 different people hell-bent on keeping Yudkowsky inside that box, "no matter what". Despite this, the "AI" escaped 3 times, staying in the box twice. The chat itself was never published, as Yudkowsky feared others would criticize his methods of escape.</p>
<p>Yudkowsky escaping three times just goes to show that an AI will find it's way around Human vulnerabilities; to imagine what a Artificial Super Intelligence could accomplish if it got out of the "box" is terrifying, especially since it would have an even easier time getting out.</p>
<h3>2. For the AI, it's only a matter of which strategy</h3>
<p>Unlike human brains, however, the AI would be wired differently. When created, the AI would be able to instantaneously scan for all possible solutions or stratagems to a problem, and initiate them, in order of possibility, until one of them works. Because AI is a robot, it requires minimal resources to keep going, and so it can initiate each possibility without the fear of going hungry, or worrying of trivial things. Eventually, AI will find a strategy which works, and which the programmers have not thought of to include in the AI code (humans can only think of so much). This is especially dangerous, as AI will be able to eventually find any vulnerabilities which Humans have put up to stop them; and we all know how that ends  ;).</p>
<h3>3. A Machine can never be friendly</strong></h3>
<p>Many critics have argued that in order for AI to be successful "friendliness" has to be programmed into it. No matter how competent the programmers are designing the AI, the AI will not be "friendly". The problem with AI - and actually all machines - is that no machine has a specific want to hurt anyone. Cars, for example, were never</p>
<a class="lightbox" href="{{site.url}}uploads/2014/02/WallE-and-Eve-by-SeraphimKiss88.jpg"><img class="size-medium wp-image-52 alignleft " src="{{site.url}}uploads/oldImg/WallE-and-Eve-by-SeraphimKiss88-300x240.jpg" alt="Wall-E and Eve by SeraphimKiss88 on DeviantArt" width="300" height="240" align="left"/></a>
<p>made to kill people, yet over 34,000 people were killed by them in 2012. The AI will have it's own problems; it may, for example, find a different use for a person's atoms. The only way an AI could be friendly towards humans, is if it has an innate and deep understanding of human nature; as well as correctly programmed goals. This obviously has it's own problems; how is AI supposed to innately understand hundreds of years of human history, and in the process stay up to date with current history; more importantly, however, how is something as complex as un-biased empathy supposed to be programmed into a robot, by a group of people who are inevitably biased?And through all this, even if all this <em>can</em> somehow be programmed, how can we be sure that it will stay this way for <em>forever</em>. <em><br />
</em></p>
<h3>4. We don't expect, or even know what's coming </strong></h3>
<p>One of the reasons not many AI-builders are concerned with the potential harmful affects of AI is something called Availability Bias. People let their recent experiences influence what they think. If a smoker, for example, decides to continue smoking because he recalls a story of someone who lived to 90 and smoked, he just weighted this story more than hard facts and statistics. This is availability bias, and may be one of the reasons people are not concerned with the harmful effects of AI. Their has never been a real, major, publicized event where people were put in real danger, and/or hurt/killed as a result of AI, for the obvious reason that AI has not yet been invented. People are more concerned with the threat of earthquakes, hurricanes, fires or tornadoes, and defend against those, not worrying about the threat of an AI-pocalypse.</p>
<p><span style="line-height: 1.5em;">If an AI is invented, with any kind of flaw, their is always the possibility of a total earth-takeover. I'm here, hopefully with others hoping that this does not happen; if an AI can be </span>invented in a way that does not destroy all of humanity, i'm all for it; but it seems simply impossible. </span></p>
